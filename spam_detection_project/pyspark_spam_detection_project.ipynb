{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "451f2aac-5f01-41f8-9288-cdf501d65670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spam Detection Project\n",
    "(Natural Language Processing)\n",
    "\n",
    "### 01. Data science project implementation\n",
    "\n",
    "\n",
    "Unwanted messages, also known as spam, cause email users to waste a lot of time. In addition, they can be a danger to information security because, in some cases, they contain malicious links with malware. To prevent these crimes, it is necessary to develop a security system that detects spam.\n",
    "\n",
    "The goal of the project is to create a spam detection system.\n",
    "\n",
    "The dataset consists of text messages from volunteers in a study in Singapore and some spam text messages from a UK reporting site mixed together.\n",
    "\n",
    "To complete this project all the data was studied and maching learning models was created to do the predictions. <br/> \n",
    "All models was used using pyspark with Spark's MLlib. <br/>\n",
    "The data used is in the file \"SMSSpamCollection\" in csv format and, after being received, was processed for later use by ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b91261c-4588-4c4f-9750-e58b0b8d242b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import related packages\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "# Feature Selection\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Build the Model\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Start spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306199cc-2157-4687-a2c9-f9bbd509007a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 02. Sourcing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfbfd39-5b3d-4545-894f-b31baceec109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|class|text                                                                                                                                                       |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |\n",
      "|ham  |Ok lar... Joking wif u oni...                                                                                                                              |\n",
      "|spam |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 02.1 Loading data\n",
    "data = spark.read.csv('/FileStore/tables/SMSSpamCollection', inferSchema=True, sep='\\t') # \\t it is separated by tabs, not by commas\n",
    "data = data.withColumnRenamed('_c0', 'class').withColumnRenamed('_c1', 'text')\n",
    "\n",
    "data.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1e2465-667e-4954-8098-d764e06e048a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 03. Exploratory Data Analysis and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2c64d2-89d8-402f-b215-01b28de47660",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham| 71.4545266210897|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('length', length(data['text']))\n",
    "\n",
    "data.show(3)\n",
    "data.groupBy('class').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f6882e-c567-4fb4-acb7-a53c7e6efef3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 04. Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8950e268-4f53-47fa-a217-5233d75df21c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 04.1. Data configuration\n",
    "\n",
    "# 04.1.1 Class index and setting up features\n",
    "ham_spam_to_numeric = StringIndexer(inputCol='class', outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
    "stop_remove = StopWordsRemover(inputCol='token_text', outputCol='stop_token')\n",
    "count_vec = CountVectorizer(inputCol='stop_token', outputCol='c_vec')\n",
    "idf = IDF(inputCol='c_vec', outputCol='tf_idf') # Inverse document frequency\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf', 'length'], outputCol='features')\n",
    "\n",
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_numeric, tokenizer, stop_remove, count_vec, idf, clean_up])\n",
    "clean_data = data_prep_pipe.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5d6894-3906-4d99-8d11-1c1e50ecee96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+-----+\n",
      "|class|                text|length|label|\n",
      "+-----+--------------------+------+-----+\n",
      "|  ham|Go until jurong p...|   111|  0.0|\n",
      "+-----+--------------------+------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+--------------------+------+-----+--------------------+\n",
      "|class|                text|length|label|          token_text|\n",
      "+-----+--------------------+------+-----+--------------------+\n",
      "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|\n",
      "+-----+--------------------+------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+\n",
      "|class|                text|length|label|          token_text|          stop_token|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+\n",
      "|class|                text|length|label|          token_text|          stop_token|               c_vec|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                text|length|label|          token_text|          stop_token|               c_vec|              tf_idf|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                text|length|label|          token_text|          stop_token|               c_vec|              tf_idf|            features|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|(13424,[7,11,31,6...|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 04.1.2 Inside the Data Configuration (Step-by-step and results)\n",
    "\n",
    "step_1 = ham_spam_to_numeric.fit(data).transform(data)\n",
    "step_1.show(1)\n",
    "\n",
    "step_2 = tokenizer.transform(step_1)\n",
    "step_2.show(1)\n",
    "\n",
    "step_3 = stop_remove.transform(step_2)\n",
    "step_3.show(1) # .select('token_text', 'stop_token')\n",
    "\n",
    "step_4 = count_vec.fit(step_3).transform(step_3)\n",
    "step_4.show(1)\n",
    "\n",
    "step_5 = idf.fit(step_4).transform(step_4)\n",
    "step_5.show(1) # .select('c_vec', 'tf_idf')\n",
    "\n",
    "step_6 = clean_up.transform(step_5)\n",
    "step_6.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44474b02-5311-49c0-9100-fbf86a4a96f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4.1.3 Restructuring between predictor and target attributes\n",
    "data_models = clean_data.select('label', 'features')\n",
    "\n",
    "# 4.1.4 Dividing the sample\n",
    "train_data, test_data = data_models.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7da35f7-685a-4da4-baef-ece760792825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,7,8...|[24.5166164970199...|[0.99999999997748...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,15,...|[22.1057883431354...|[0.99999999974905...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[21.6815800738554...|[0.99999999961646...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ACC of Logistic Regression Model: 0.9789464169061226\n",
      "Out[24]: 0.9794188861985472"
     ]
    }
   ],
   "source": [
    "# 04.2. Modeling: Logistic Regression (Test 01)\n",
    "\n",
    "# 4.2.1. Building a Linear Regression Model object\n",
    "model_log_r = LogisticRegression(featuresCol='features', labelCol='label', predictionCol='prediction') \n",
    "spam_detector = model_log_r.fit(train_data)\n",
    "\n",
    "# 4.2.2. Run model\n",
    "test_predictions = spam_detector.transform(test_data)\n",
    "test_predictions.show(3)\n",
    "\n",
    "# 4.2.3. Checking the efficiency of the model\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_predictions)\n",
    "print('ACC of Logistic Regression Model:', acc) # R: 0.9789\n",
    "\n",
    "evaluator = spam_detector.evaluate(test_data)\n",
    "evaluator.accuracy # R: 0.9794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbb4807-1569-46fa-a540-12ebbcfee922",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------+-----------+----------+\n",
      "|label|            features|rawPrediction|probability|prediction|\n",
      "+-----+--------------------+-------------+-----------+----------+\n",
      "|  0.0|(13424,[0,1,2,7,8...|  [14.0,21.0]|  [0.4,0.6]|       1.0|\n",
      "|  0.0|(13424,[0,1,7,15,...|  [14.0,21.0]|  [0.4,0.6]|       1.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|  [14.0,21.0]|  [0.4,0.6]|       1.0|\n",
      "+-----+--------------------+-------------+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ACC of Decision Tree Model: 0.9355091346302511\n"
     ]
    }
   ],
   "source": [
    "# 04.3. Decision tree classifier (Test 02)\n",
    "\n",
    "# 4.3.1. Building a Linear Regression Model object\n",
    "model_dtc = DecisionTreeClassifier(featuresCol='features', labelCol='label', predictionCol='prediction') \n",
    "spam_detector = model_dtc.fit(train_data)\n",
    "\n",
    "# 4.3.2. Run model\n",
    "test_predictions = spam_detector.transform(test_data)\n",
    "test_predictions.show(3)\n",
    "\n",
    "# 4.3.3. Checking the efficiency of the model\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_predictions)\n",
    "print('ACC of Decision Tree Model:', acc) # R: 0.9355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fd6d419-03d7-439d-b00e-5dbc048dc872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,7,8...|[1.83135196933833...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,15,...|[1.70028581914278...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[1.86016095733911...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ACC of SVC Machine Model: 0.9829266416427664\n",
      "Out[26]: 0.9830508474576272"
     ]
    }
   ],
   "source": [
    "# 04.4. Support Vector Machine (Test 03)\n",
    "\n",
    "# 4.4.1. Building a Linear Regression Model object\n",
    "model_svc = LinearSVC(featuresCol='features', labelCol='label', predictionCol='prediction') \n",
    "spam_detector = model_svc.fit(train_data)\n",
    "\n",
    "# 4.4.2. Run model\n",
    "test_predictions = spam_detector.transform(test_data)\n",
    "test_predictions.show(3)\n",
    "\n",
    "# 4.4.3. Checking the efficiency of the model\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_predictions)\n",
    "print('ACC of SVC Machine Model:', acc) # R: 0.9829\n",
    "\n",
    "evaluator = spam_detector.evaluate(test_data)\n",
    "evaluator.accuracy # R: 0.9830"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006cf1ed-790b-4ca6-aae2-835450faabef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,7,8...|[-790.42125251548...|[1.0,1.0546815041...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,15,...|[-661.25069354602...|[1.0,1.1702857962...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[-542.13806409330...|[1.0,2.5887078920...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ACC of NB Model: 0.925633960821264\n"
     ]
    }
   ],
   "source": [
    "# 04.5 Modeling: Naive Bayes (Test 04)\n",
    "\n",
    "# 4.5.1. Building a Gaussian NB Model object\n",
    "model_nb = NaiveBayes()\n",
    "spam_detector = model_nb.fit(train_data)\n",
    "\n",
    "# 4.5.2. Run model\n",
    "test_predictions = spam_detector.transform(test_data)\n",
    "test_predictions.show(3)\n",
    "\n",
    "# 4.5.3. Checking the efficiency of the model\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_predictions)\n",
    "print('ACC of NB Model:', acc) # R: 0.9256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4649a15d-6764-4b46-9f76-352bba40b135",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The ML models with the best performance were Logistic Regression and SVC Machine, which achieved excellent scores. <br/>\n",
    "New tests will then be performed using them to understand which is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117a25ef-2690-4bfd-ab3b-531b28b511d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: (0.9789464169061226, 0.9829266416427664)"
     ]
    }
   ],
   "source": [
    "# 04.9. Final performance test\n",
    "\n",
    "resultados_log_regressor_cv, resultados_svc_machine_cv = [], []\n",
    "\n",
    "for i in range(30):\n",
    "    model_lr = LogisticRegression(featuresCol='features', labelCol='label', predictionCol='prediction') \n",
    "    spam_detector = model_lr.fit(train_data)\n",
    "    acc_eval = MulticlassClassificationEvaluator()\n",
    "    resultados_log_regressor_cv.append(acc_eval.evaluate(spam_detector.transform(test_data)))\n",
    "\n",
    "    model_svc = LinearSVC(featuresCol='features', labelCol='label', predictionCol='prediction')\n",
    "    spam_detector = model_svc.fit(train_data)\n",
    "    acc_eval = MulticlassClassificationEvaluator()\n",
    "    resultados_svc_machine_cv.append(acc_eval.evaluate(spam_detector.transform(test_data)))\n",
    "    \n",
    "resultados_log_regressor_cv = np.array(resultados_log_regressor_cv)\n",
    "resultados_svc_machine_cv = np.array(resultados_svc_machine_cv)\n",
    "\n",
    "resultados_log_regressor_cv.mean(), resultados_svc_machine_cv.mean()\n",
    "# (0.9789464169061226, 0.9829266416427664)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3dfd328-07f4-4388-a778-b37507508fd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 05. Discussion and Conclusion\n",
    "According to the results of the models, the algorithm that achieved the best result compared to the other algorithms was Linear Support Vector Machine, with an accuracy of around 98%.\n",
    "\n",
    "Thank you for following up here and if you have any suggestions or constructive criticism, I'm 100% open!\n",
    "\n",
    "Joao Ambrosio"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_11_nlp_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
